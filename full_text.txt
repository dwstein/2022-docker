version: '3.8'

services:
  backend-service:
    build: .
    image: rag3-image
    container_name: rag3-backend-container
    command: uvicorn app.main:app --host 0.0.0.0 --port 80 --reload
    ports:
      - "8080:80"
    volumes:
      - .:/app
    networks:
      - app-network

  ollama-service:
    image: ollama/ollama
    container_name: ollama-container
    volumes:
      - ollama:/root/.ollama
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  ollama:
    driver: local


# app/main.py


from fastapi import FastAPI
from .routes.endpoints import router as api_router

app = FastAPI(title="My FastAPI Application")

# Include all routers
app.include_router(api_router)


# app/routes/endpoints.py

from typing import Optional
import uuid
import logging
from fastapi import APIRouter, HTTPException, Depends, Response

from ..services.api_services import query_ollama_api
from models.models import Conversation  # Ensure correct relative import path

# Setup logger
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

router = APIRouter()

# Simulated database or in-memory store for conversations
conversations = {}

def get_or_create_conversation(conversation_id: Optional[str] = None) -> (Conversation, str):
    if conversation_id:
        if conversation_id in conversations:
            logger.info(f"Retrieved existing conversation with ID {conversation_id}")
            return conversations[conversation_id], conversation_id
        else:
            logger.warning(f"Conversation ID {conversation_id} not found in existing records.")
            raise HTTPException(status_code=404, detail="Conversation ID not found")
    else:
        new_conv_id = str(uuid.uuid4())
        new_conv = Conversation()
        conversations[new_conv_id] = new_conv
        logger.info(f"Created new conversation with ID {new_conv_id}")
        return new_conv, new_conv_id


@router.get("/")
def read_root():
    logger.info("Root endpoint accessed")
    return {"message": "The server is running!"}

@router.post("/ask")
async def ask_question(response: Response, model: str, question: str, conversation_id: Optional[str] = None):
    conversation, conv_id = get_or_create_conversation(conversation_id)
    if not conversation_id:
        response.headers['X-Conversation-ID'] = conv_id  # Include the new conversation ID in the response headers
    logger.info(f"Asking question: {question} using model: {model} in conversation: {conv_id}")
    try:
        api_response = await query_ollama_api(model, question, conversation)
        logger.info("Query successful, returning response")
        return api_response
    except Exception as e:
        logger.error(f"Error in ask_question: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal Server Error")




# app/services/api_services.py


from fastapi import HTTPException
import httpx
import logging
import json
from models.models import Conversation  # Ensure you import your Conversation model

logger = logging.getLogger(__name__)

def parse_json_response(raw_json):
    """ Attempt to parse multiple JSON objects if found. """
    try:
        objs = []
        idx = 0
        while idx != len(raw_json):
            obj, idx = json.JSONDecoder().raw_decode(raw_json, idx=idx)
            objs.append(obj)
            raw_json = raw_json[idx:].strip()
            idx = 0 if raw_json else len(raw_json)
        return objs
    except json.JSONDecodeError as e:
        logger.error("Failed to decode JSON: " + str(e))
        raise

async def query_ollama_api(model: str, question: str, conversation: Conversation) -> dict:
    url = "http://ollama-container:11434/api/chat"
    payload = {
        "model": model,
        "messages": conversation.get_history() + [{"role": "user", "content": question}],
        "stream": False
    }
    try:
        async with httpx.AsyncClient(timeout=20.0) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            try:
                # First try parsing normally
                answer = response.json()
            except json.JSONDecodeError:
                # If normal parsing fails, attempt to parse multiple JSON objects
                answer = parse_json_response(response.text)
                if not answer:
                    raise HTTPException(status_code=500, detail="Invalid JSON response")
            # Assuming the relevant information is in the first JSON object if multiple were parsed
            first_answer = answer[0] if isinstance(answer, list) else answer
            conversation.add_message("user", question)
            conversation.add_message("llm", first_answer.get("message", {}).get("content", ""))
            return first_answer
    except httpx.HTTPStatusError as http_err:
        logger.error("HTTP status error occurred: {0}".format(http_err))
        raise HTTPException(status_code=http_err.response.status_code, detail=str(http_err.response.text))
    except httpx.RequestError as req_err:
        logger.error("Request failed: {0}".format(req_err))
        raise HTTPException(status_code=500, detail="Network error")


fastapi
pydantic
uvicorn
watchfiles
httpx
langchain

FROM python:latest

# set the working directory
WORKDIR /app

# install dependencies
COPY ./requirements.txt /app
RUN pip install --no-cache-dir --upgrade -r requirements.txt

# copy the scripts to the folder
COPY . /app

# start the server
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "80"]
from pydantic import BaseModel
from typing import List, Optional

class Message(BaseModel):
    role: str  # 'user' or 'llm'
    content: str

class Conversation(BaseModel):
    messages: List[Message] = []

    def add_message(self, role: str, content: str):
        self.messages.append(Message(role=role, content=content))

    def get_history(self):
        return self.messages

# app/models/channel.py

import json
from dataclasses import dataclass, field

@dataclass
class Channel:
    id: str
    name: str
    tags: list[str] = field(default_factory=list)
    description: str = ""

def load_channels(filepath: str) -> dict[str, Channel]:
    with open(filepath, encoding="utf8") as file:
        channels_raw = json.load(file)
        channels = {item['id']: Channel(**item) for item in channels_raw}
    return channels

channels = load_channels("channels.json")


### rag3 - Containerized LLM for Python

### Current Issues
- conversation id issues
- issue seems to be with this fuction get_or_create_conversation in endpoints
- the idea is to have a consistent conversation id.  



## Project Description

Create an application that uses LLMs to read my documents and researches them.  I want the system to be private so I’m comfortable sharing my personal information like legal documents, bank statements…etc.  I also want to choose different models, including the latest LLMs and embedding models to have the best possible experience.


### Project Info

## requiremetns.txt
```bash
fastapi
pydantic
uvicorn
watchfiles
httpx
langchain
```


### Project Usage

```
pip install -r requirements.txt
```

This is how you run the code locally (without Docker):

```
uvicorn main:app --host 0.0.0.0 --port 8080 --reload
```

Build and run the Docker image locally, as follows:

```
docker build -t channel-api .
docker run -d -p 8080:80 channel-api
```

In order to run the example server with docker compose, use this:

```
docker-compose up --build
docker-compose up -d 
```

### Prompt Info

## run_full_text.sh
- `chmod +x run_full_text.sh`
- `./run_full_text.sh`




### Project Directory - Goal
```bash
.
├── Dockerfile
├── README.md
├── app
│   ├── __init__.py
│   ├── main.py
│   ├── routes
│   │   ├── __init__.py
│   │   └── endpoints.py
│   └── services
│       ├── __init__.py
│       └── api_services.py
├── channels.json
├── directory_tree.txt
├── docker-compose.yaml
├── full_text.txt
├── models
│   ├── __init__.py
│   ├── channel.py
│   └── models.py
├── requirements.txt
└── run_full_text.sh

5 directories, 17 files
```
